# ğŸŒ¦ï¸ETL Weather Pipeline with AWS deployment

## ğŸš€Overview
The **ETL Weather Pipeline** is designed to extract weather data from an API, transform it into a structured format, and load it into a PostgreSQL database using **Apache Airflow**. 
The project follows an **Agile methodology**, breaking down tasks into sprints.

## ğŸ”„ETL Process
The ETL process consists of three main stages:
1. **Extract** - Fetch weather data from an API.
2. **Transform** - Preprocess and structure the extracted data.
3. **Load** - Store the transformed data into a PostgreSQL database.

## ğŸ“¥Architecture
- **Source Data**: APIs, Internal Databases, IoT Devices.
- **Processing**: Preprocessing & Transformation (JSON format).
- **Storage**: PostgreSQL, MongoDB, SQL databases.
- **Workflow Management**: Apache Airflow for scheduling & execution.
- **Containerization**: Docker for deployment.

## ğŸ› ï¸Project Structure
```
Data Pipeline
    â”œâ”€â”€ API (HTTP Operator)
    â”œâ”€â”€ Transformation (Python Code)
    â”œâ”€â”€ Load (PostgreSQL)
```
- Uses **Airflow Connection** to interact with APIs.
- **PythonOperator** for transformation logic.
- Stores data in **Dockerized PostgreSQL**.

## Steps to Run the ETL Pipeline
### 1ï¸âƒ£ Setup Environment
- Ensure **Docker** and **Airflow** are installed.
- Install **DBeaver** 

### 2ï¸âƒ£ Start Airflow
```bash
astro dev start
```
- Navigate to **Admin > Connections**
- Add **Postgres connection**.
- Add **Open Meteo API connection**.
- Run the `weather_etl_pipeline`.

### 3ï¸âƒ£ Access Database using DBeaver
- Open **DBeaver**.
- Select **PostgreSQL** as database.
- Open **SQL Editor**.
- Run the query:
  ```sql
  SELECT * FROM weather_data;
  ```

### 4ï¸âƒ£ Deploy using Terminal
```bash
astro login
astro deploy
```

### 5ï¸âƒ£ Deploy to AWS RDS
- Go to **AWS Console** > **RDS**.
- Create a **PostgreSQL database**.
- Open the database and configure:
  - **VPC Security Groups**.
  - **Security Group ID**.
  - Edit **Inbound Rules** to allow access.
  - Copy the **Database Endpoint**.

## Technologies Used
âœ… **Apache Airflow** - Task Scheduling & Execution.
âœ… **PostgreSQL** - Data Storage.
âœ… **Python** - ETL Scripting.
âœ… **Docker** - Containerization.
âœ… **AWS RDS** - Cloud Database.
âœ… **DBeaver** - SQL Query Execution.

## â˜ï¸Conclusion
This ETL pipeline automates weather data extraction, transformation, and loading, ensuring seamless data management for analysis and insights.


